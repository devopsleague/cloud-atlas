.. _gguf:

=====================
GGUF格式(大模型文件)
=====================

在大模型领域，GGUF是一个非常常见的大模型文件格式，也是HuggingFace官方主推的大模型格式。常见的大模型预训练结构大多数会提供GGUF版本。

大语言模型的开发通常使用PyTorch等框架，其预训练结果通常也会保存为相应的二进制格式: ``pt`` 后缀的文件通常是 :ref:`pytorch` 框架保存的二进制预训练结果。

大模型的存储一个很重要的问题是它的模型文件巨大，而模型的结构、参数等也会影响模型的推理效果和性能。为了让大模型更加高效的存储和交换，就有了不同格式的大模型文件。其中，GGUF就是非常重要的一种大模型文件格式:

- GGUF文件全称是GPT-Generated Unified Format，是由Georgi Gerganov( ``llama.cpp`` 开源项目创始人)定义发布的一种大模型文件格式
- GGUF就是一种二进制格式文件的规范: 采用了多种技术来保存大模型预训练结果，包括采用紧凑的二进制编码格式、优化的数据结构、内存映射等
- ``llama.cpp`` 官方提供了转换脚本，可以将 ``pt`` 格式的预训练结构以及 ``safetensors`` 模型文件转换成 ``GGUF`` 格式的文件，并且在转换时可以选择 ``量化参数`` ，降低模型的资源消耗。 **这个过程性能损失很低!**
- ``GGUF`` 包含了加载模型所需要的所有信息，无需依赖外部文件。这大大简化了模型部署和共享的过程。

GGUF格式大模型文件能够更快载入模型的主要原因:

- 二进制格式: 比文本格式文件可以更改读取和解析(文件更紧凑，减少了读取和解析时所需的I/O操作和处理时间)
- 优化的数据结构: 为快速访问和加载模型数据提供了支持(例如数据可能按照内存加载的需要进行组织以减少加载时的处理)
- 内存映射( ``mmap`` )兼容性: 如果GGUF支持内存映射（mmap），就允许直接从磁盘映射数据到内存地址空间，从而加快了数据的加载速度。并且这种方式，数据可以在不实际加载整个文件的情况下被访问，对于大模型非常有效
- 高效的序列化和反序列化: GGUF可能使用高效的序列化和反序列化方法，这意味着模型数据可以快速转换为可用的格式
- 少量的依赖和外部引用: 如果GGUF格式设计为自包含，即所有需要的信息都存储在单个文件中，这将减少解析和加载模型时所需的外部文件查找和读取操作
- 数据压缩: GGUF格式可能采用了有效的数据压缩技术，减少了文件大小，从而加速了读取过程
- 优化的索引和访问机制: 文件中数据的索引和访问机制可能经过优化，使得查找和加载所需的特定数据片段更加迅速

在最新版本的llama.cpp中，已经去除了对GGML的支持，因此未来GGUF才是大模型文件格式的主流（在llama.cpp生态中）

参考
======

- `GGUF格式的大模型文件是什么意思？gguf是什么格式？如何使用？为什么有GGUF格式的大模型文件？GGUF大模型文件与GGML的差异是啥？ <https://www.datalearner.com/blog/1051705718835586>`_
